# you can move this content to front matter of [language]/tabs/projects.md
###########################################################
#                Projects Page Data
###########################################################
page_data:
  main:
    header: "Data Science Projects"
    info: "Our job is extract signal from noise"
    text_color: "white"
    # if you don't want to use background image, comment it. back_color will be activated.
    img: ":projects-heading.jpg"
    back_color: "lightblue"

  category:
    - title: "Cloud"
      type: id_example
      color: "gray"
    #- title: "Picture"
     # type: id_picture
      #color: "#62b462"
    - title: "Kaggle Competition"
      type: id_quote
      color: "#5733FF"

  list:
    # example
    - type: id_example
      project_name: "Inventory Management and Demand Forecasting by Google Cloud Platform"
      project_excerpt: "Role: Technical Lead"
      img: ":project1_thumb.jpg"
      img_title: "img title1"
      date: "2021-03-13"
      post: |

        # Abstract: 
        The project introduces a comprehensive solution for enhancing inventory management in the e-commerce domain. Leveraging the robust capabilities of Google Cloud Platform, our system integrates Big Data and Machine Learning to optimize inventory levels. The core of our analytics pipeline resides in Google BigQuery, where data summarization and analysis take place, providing dynamic insights into inventory availability and accurate demand forecasting. The culmination of our efforts is a user-friendly dashboard developed on Google's Looker Studio. This platform empowers end-users and clients to intuitively visualize product inventory counts and forecasts, facilitating informed decision-making and efficient inventory control.

        # Dataset 

        The [eCommerce dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce?project=galvanic-portal-404814) contains information about customers, products, orders, logistics, web events and digital marketing campaigns. The contents of this dataset are synthetic, and are provided to industry practitioners for the purpose of product discovery, testing, and evaluation.
        # Improve inventory management with sales forecasting deployed by BigQuery ML and AI Platform Training & Prediction

        In the e-commerce sector, effectively managing inventory is crucial: you need to maintain the right balance, ensuring you have enough stock without overstocking. For large e-commerce companies, this involves managing inventory levels for a vast array of products.

        To guide inventory decisions, historical sales data is invaluable. By analyzing past customer purchase patterns, you can predict future buying trends, helping to determine optimal inventory levels. Time series forecasting is an essential tool in this process.

        For data science teams supporting e-commerce inventory management, this entails generating numerous forecasts and managing the infrastructure required for machine learning (ML) model development and deployment. However, by utilizing BigQuery ML in Google Cloud Platform (GCP), you can streamline this process. BigQuery ML allows you to train, evaluate, and deploy ML models directly using SQL statements, bypassing the need for a separate ML infrastructure. This approach can significantly save time and resources in your e-commerce sales forecasting project.



        # Set up time series modeling works in BigQuery ML
        A time series model with BigQuery ML, multiple components are involved, including an Autoregressive integrated moving average (ARIMA) model. For this project, The BigQuery ML model creation pipeline uses the following components:

        Pre-processing: Automatic cleaning adjustments to the input time series, inspect the most appropriate training data which addresses issues like missing values, duplicated timestamps. There are multiple levels we can also consider for time series, easily customized function on demand:
        1. Product category level
        2. Specific products level
        3. Store level

        Holiday effects: Time series modeling in BigQuery ML can also account for holiday effects. By default, holiday effects modeling is disabled. But since this data is from the United States, and the data includes a minimum one year of daily data, you can also specify an optional HOLIDAY_REGION. 
        Seasonal and trend decomposition using the Seasonal and Trend decomposition using Loess (STL) algorithm. Seasonality extrapolation using the double exponential smoothing (ETS) algorithm.
        Trend modeling using the ARIMA model and the auto.ARIMA algorithm for automatic hyper-parameter tuning. In auto.ARIMA, dozens of candidate models are trained and evaluated in parallel. The best model comes with the lowest Akaike information criterion (AIC).
        We can use a single SQL statement to train the model to forecast a single product or to forecast multiple products at the same time. For more information, see The CREATE MODEL statement for time series models.



        # Deployment quick guide:
        1. Create a new project on GCP platform
        2. Enable BigQuery, Vertex AI, AI platform, LookerStudio services and create new instance and new notebook with python kernel -- You are ready to run sales forecast
        3. Prepare training dataset
        3.1. Upload/Import dataset in Cloud storage and BigQuery, create training dataset under your project or
        3.2. Enable Streamming by Dataflow for data training preparation
        4. Develop and train model followed by making prediction
        5. Export output as predicted sales for further usage as monitoring dashboard

        Here is the example how we can integrate forecasted sales into current dashboard:

        <img src="[https://i.ibb.co/RhC6V4R/googletrends-81-1-1.png](https://ibb.co/51NCZF8"><img src="https://i.ibb.co/D1xmdpC/Picture1.png)" alt=""></a>
          </td>



        ## Suggestion for further deployment
        1. Adding lagging and rolling features to the model for each training window
        2. Considering using advanced models like XGBoost or LightGBM 
        3. Adding insightful features like Top 25 Rising queries from **Google Trends** <td>
        <a href="https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/google-search-trends?_ga=2.261190030.2019434361.1656948847-1975246695.1656948843&project=galvanic-portal-404814">
          <img src="https://i.ibb.co/RhC6V4R/googletrends-81-1-1.png" alt=""></a>[dataset Google trends on Google BigQuery](https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/google-search-trends?_ga=2.261190030.2019434361.1656948847-1975246695.1656948843&project=galvanic-portal-404814)
            

        ## Docoments
        The [Presentation Recording:](https://drive.google.com/file/d/1Roq84Chgowoh7PfVIuHarL1su3gnZMOA/view?usp=drive_link) and the open-source [Interactive dashboard:](https://lookerstudio.google.com/reporting/ebea5752-963e-4a55-aa45-ebb846519557/page/p_057twy79bd?s=nIzjhrMlO8g)
        the flyers and technical document of the project can be found [here] (https://github.com/trungle14/GoogleCloud_InventoryManagement/tree/main) 

    # example
    - type: id_quote
      project_name: "Walmart Retail Sales Forecasting - M5 Accuracy Competition"
      project_excerpt: "Model used: LightGBM, XGBoost, Deep Neural Network"
      img: ":project_walmart.png"
      img_title: "img title5"
      date: "2023-12-05"
      post: |
         ## 1	Overview
         ### 1.1	Description of Project

          This is a prediction problem  project for Walmart (A Top Retail Group) Sales dataset from Kaggle for the unit sales forecasting. Advanced and comprehensive analytics skills, including Exploratory Data Analysis and Machine Learning Data Prediction Analysis techniques will be used in this case for generating data-driven business insights.

         ### 1.2	Business Context


          In the dynamic landscape of the retail industry, the ability to predict sales accurately is paramount for sustaining and enhancing business operations. For a retail giant like Walmart, whose vast operations span a multitude of products, locations, and customer segments, the challenge of forecasting sales becomes even more intricate.
          Challenges and Risks:
          Walmart confronts the formidable task of maximizing decision-making efficiency amid a sea of data. The stakes are high, as inaccurate predictions can lead to substantial losses. Traditional prediction methods, once reliable, now struggle to cope with the complexities of modern retail dynamics. To avoid costly mistakes and enhance forecasting accuracy, there is a pressing need for the integration of cutting-edge data science techniques.
          Business Imperatives:
          Precise sales predictions stand as the linchpin in Walmart's strategy to navigate both realized and potential revenue opportunities. Efficient inventory management, customer satisfaction, strategic promotions, and a competitive edge hinge on the ability to foresee market trends accurately.
          Benefits of Sales Prediction:\
          (1)	Efficient Inventory Management: Anticipate demand trends, reducing stockouts and overstocks.\
          (2)	Customer Satisfaction: Ensure product availability, meeting customer expectations.\
          (3)	Smart Promotions: Strategically plan promotions based on predictive insights.\
          (4)	Competitive Edge: Stay ahead by responding swiftly to market shifts.\
          (5)	Optimized Supply Chain: Streamline operations for cost-effective supply chain management.\
          (6)	Support for Strategic Decisions: Informed decision-making for sustained growth.\
          (7)	Reduce Financial Risks: Improve budget management efficiency through accurate sales forecasts.\
          (8)	Raise Shareholder Confidence: Provide stakeholders with reliable projections, enhancing trust.

          **Situation:**
          Walmart is at the nexus of leveraging its rich dataset to drive decision-making efficiency. The precision of sales predictions becomes pivotal, steering the company away from both tangible and missed revenue opportunities.
          Key Question:
          How can Walmart forecast daily sales for the next 28 days, leveraging hierarchical sales data effectively?
          Proposed Solution:
          The proposed solution involves harnessing the power of machine learning to predict future sales. By embracing advanced analytics, we  aim to enhance the  forecast accuracy for Walmart, ensuring a proactive and data-driven approach to sales management.

          This strategic integration of data science not only addresses current challenges but positions Walmart at the forefront of innovative and efficient retail practices, fostering sustained growth and market leadership.

         ## 2. Data Description & Exploratory Data Analysis

          <img width="670" alt="Screenshot 2024-01-04 at 19 59 54" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/ca4cde95-c4f4-4399-8041-071ab7ac8683">

          This table shows the overview of the Input Data:
          Raw Data	Description	# Feature	# Record	Data Size
          calendar.csv	Workday & Special event day (e.g. SuperBowl)	14	1.9 K	103 kB 
          sell_prices.csv	Price of the products sold per store and date	4	6.84 M	203.4 MB
          sales_train_validation.csv	historical daily unit sales data per product and store [d1 - d1913]	1019	30.5 K	120 MB
          sales_train_evaluation.csv	sales [d1 - d_1941]	1047	30.5 K	121.7 MB
          Based on the structure of data we see the data would be of below format:

          <img width="685" alt="Screenshot 2024-01-04 at 20 00 36" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/d5acb20b-a021-4245-8f3d-27e581c4b1a9">


         ## 3. Methodology 


          <img width="660" alt="Screenshot 2024-01-04 at 22 49 07" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/56702eff-a1f8-4fbe-ad85-ceb02ba3dde6">




          **3.3.1. Price feature**\
          We are doing feature engineering here to get price related data, we have week wise data of price (we have price features for test weeks as well).
          We are using expanding max price , minimum price , standard deviation , mean, so that there is no data   leakage from future to past, and ,model can solely use the past data using expanding method (since the data is already sorted time wise we are not sorting again , saves in computation time).


          **3.3.2. Calendar features**\
          we see prices of some items starting for a particular week, which might indicate that would be release week for the product so we can use data in base data frame after that point (as since earlier data was in long format it would have data for all items through all days)
          This reduces the size of the data and will have features of when the product was released (capturing any trends if items get sold when we are predicting for volumes closer to release dates). Then we do label encoding of the categorical features so that they can be used for regression algorithms


          **3.3.3. Lag and rolling lags features**\
          Another important feature we observed in winning solutions is they used lags data and roll data in feature engineering. This gives how trends data could be captured using a regression algorithm , though we are not specifically using time series data.
          For this we have considered rolling sum of the number of times, 0 units of product were sold, 7, 14, 30, 60, 180 days of roll (week, 2 weeks, approx month, 2 months approx, approx half year), with this we will be able to capture trend details.
          As next important features we have chosen lag features (these will capture sales with a lag of that many days we have in feature.


          **3.3.4. Categories - Item, Store, Department, State Level Features**\
          We then use category wise sales data, item wise sales data, department wise sales data (across all stores), then also use store and category wise sales data, store and item wise sales data, store and department wise sales data. This gives cross sectional features that our model could pick if there is any trend.



          **4. Model Training and Prediction**\

          **Train and Predict**\
          First we need to model comparison to see which model produces a better kaggle score and use that model , then optimize the step size so as to improve the score further.
          through this process we are basically using the sales data that we have on t- step (for example during model selection here for 1- 14 days prediction step will be 14, and from 14 till 28th day step will be 28 days)
          Then we run a prediction model where we first loop over store and department to train the model (slow) , next over store and category (will be quicker) and take average of both the methods to arrive at final submission.



          **LightGBM**\
          <img width="1080" alt="Screenshot 2024-01-06 at 22 29 14" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/6f93c326-e9eb-41cf-b1db-3ce0eec7ffaf">

          **Extreme Gradient Boosting - XGBOOST**\
          <img width="1084" alt="Screenshot 2024-01-06 at 22 30 52" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/19b07cc1-9c98-4cd2-9cd8-46d53e6599f6">


          **Neural network**\
          <img width="784" alt="Screenshot 2024-01-06 at 22 37 13" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/86d7c335-5570-47e0-98b1-4ec673617643">






          | Models    | Hyperparameters                           | Kaggle Score |
          |-----------|-------------------------------------------|--------------|
          | LightGBM  | [See Hyperparameters](#lightgbm-parameters)| 0.5302        |
          | XGBoost   | [See Hyperparameters](#xgboost-parameters)| 0.5599      |
          | Neural Netwwork| [See Hyperparameters](#lightgbm-parameters)| 0.728 |

          ## LightGBM Parameters

          ```python
          lgb_params = {
              'boosting_type': 'gbdt',
              'objective': 'tweedie',
              'tweedie_variance_power': 1.1,
              'metric': 'rmse',
              'subsample': 0.5,
              'subsample_freq': 1,
              'min_child_weight': 1,
              'learning_rate': 0.03,
              'num_leaves': 2 ** 11 - 1,
              'min_data_in_leaf': 2 ** 12 - 1,
              'feature_fraction': 0.5,
              'max_bin': 100,
              'n_estimators': 1400,
              'boost_from_average': False,
              'verbosity': -1
                              }
          Lgbm = LGBMRegressor(**lgb_params)
          callbacks = [early_stopping(stopping_rounds=50, first_metric_only=False)]
          ```



          ## XGBoost Parameters

          ```python
           # Train

          model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(64, activation='relu', input_shape=(trainX.shape[1],)),
          tf.keras.layers.Dense(1, activation='linear') ]) # Linear activation for regression
                                          

          # Compile the model
          model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])

          # Display the model summary
          #model.summary()
          # Train the model
          history = model.fit(trainX, trainY, epochs=5, batch_size=32, validation_data=(valX, valY))

          # Make predictions on the test set
          yhat = model.predict(testX).flatten()

          preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]
          preds['sales'] = yhat
          predictions = pd.concat([predictions, preds], axis=0)
          ```



          xgb_params = {
              'objective': 'reg:tweedie',  
              'eval_metric': 'rmse', 
              'subsample': 0.5,
              'colsample_bytree': 0.5,
              'learning_rate': 0.03,
              'max_depth': 11,  
              'min_child_weight': 4096,  
              'n_estimators': 1400,
              'max_bin': 100,
              'seed': 42
                    }






    - type: id_quote
      project_name: "Image Recognition - Convolutional Network & Transfer learning"
      project_excerpt: "Model used: CNN, VG166, Xception"
      img: ":project_walmart.png"
      img_title: "img title5"
      date: "2023-11-05"
      post: |




             # ðŸŒŸ Image Classification - Cats ðŸ˜º vs Dogs ðŸ¶ Problem ðŸŒŸ
             
             
             
             ## Table of Contents
             
             1. Problem overview
             2. Data processing\
                2.1. Import Libraries\
                2.2. Preprocessing\
                     2.2.1. Extract and generate image\
                     2.2.2. EarlyStopping setting
             3. Models Training & Prediction\
                3.1. Convolutional Network\
                3.2. Convolutional Network with multiple different epoch\
                3.3. Stack model with transfer learning and convolutional network\
                3.4. Transfer learning Xception model
             4. Evaluate the model\
                4.1. Generate prediction\
                4.2. Confusion Matrix
             5. Conclusion
             
             
             
             ### 1. Problem overview 
             
             
             For an effective approach combining both Deep Neural Networks (DNNs) and Transfer Learning in classifying images of cats and dogs, you can start with a pre-trained DNN model, such as ResNet, VGG, or Inception, which has already learned                rich feature representations from a large and diverse dataset. Then, you adapt this model to our specific task (classifying cats and dogs) by fine-tuning some of its layers with our dataset of cat and dog images. This method utilizes the              advanced feature extraction capabilities of DNNs and the efficiency of Transfer Learning, enabling our model to achieve high accuracy with less training data and time.
                                                              
             Will Cukierski. (2016). Dogs vs. Cats Redux: Kernels Edition. Kaggle. https://kaggle.com/competitions/dogs-vs-cats-redux-kernels-edition
             
             
             ## 2. Data processing
                ### 2.1. Import Libraries
             
             ```python
             # This Python 3 environment comes with many helpful analytics libraries installed
             # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
             # For example, here's several helpful packages to load
             
             import numpy as np # linear algebra
             import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
             
             # Input data files are available in the read-only "../input/" directory
             # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
             
             import os
             for dirname, _, filenames in os.walk('/kaggle/input'):
                 for filename in filenames:
                     print(os.path.join(dirname, filename))
             
             # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
             # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
             
             
             import tensorflow as tf
             from tensorflow import keras
             from tensorflow.keras import layers, models, optimizers
             from keras.layers import Dense, MaxPool2D, Conv2D, Dropout, Flatten, GlobalAveragePooling2D, BatchNormalization, Activation, MaxPooling2D
             from keras.models import Sequential
             
             
             from keras.callbacks import ReduceLROnPlateau
             from keras.callbacks import EarlyStopping
             from sklearn.model_selection import train_test_split
             from sklearn.metrics import classification_report, confusion_matrix
             from tensorflow.keras.optimizers import Adam
             
             from tensorflow.keras.applications import DenseNet121
             from tensorflow.keras.applications import ResNet50V2
             from tensorflow.keras.applications import ResNet152V2
             from tensorflow.keras.applications import InceptionV3
             from tensorflow.keras.applications import Xception
             
             from tensorflow.keras.optimizers.schedules import ExponentialDecay
             from tensorflow.keras.optimizers import Adam
             
             
             import numpy as np
             import pandas as pd
             import matplotlib.pyplot as plt
             ```
             
             
                ## 2.2. Preprocessing
                ### 2.2.1. Extract and generate image
             
             ```python
             import zipfile
             train_zip='../input/dogs-vs-cats-redux-kernels-edition/train.zip'
             zip_ref=zipfile.ZipFile(train_zip,'r').extractall('./')
             
             test_zip = '../input/dogs-vs-cats-redux-kernels-edition/test.zip'
             zip_ref=zipfile.ZipFile(test_zip,'r').extractall('./')
             
             import os
             train_filenames = os.listdir('./train')
             test_filenames = os.listdir('./test')
             
             # Create DataFrame with ImageDataGenerator
             train = pd.DataFrame(columns=['path', 'label'])
             train['path'] = train_filenames
             train['label'] = train['path'].str[0:3]
             
             train.label.value_counts().plot.bar() # Balanced Data
             
             width, height = 150, 150
             trainDatagen = train_datagen.flow_from_dataframe(train, directory = './train', x_col='path', y_col='label', classes=['cat', 'dog' ],
                                                        target_size=(width,height), class_mode = 'categorical', batch_size = 16,
                                                        subset='training')
             
             valDatagen = train_datagen.flow_from_dataframe(train, directory = './train', x_col='path', y_col='label', classes=['cat','dog'],
                                                        target_size=(width,height), class_mode = 'categorical', batch_size = 16,
                                                        subset='validation')
             
             
             x, y = trainDatagen.next()
             x.shape, y.shape
             
             # Display the training data
             
             plt.figure(figsize=(15,15))
             for i in range(9):
                 img, label = trainDatagen.next()
                 plt.subplot(331+i)
                 plt.imshow(img[0])
             plt.show()
             
             # Test data
             
             test = pd.DataFrame(columns=['path'])
             test['path'] = test_filenames
             test.head()
             
             test_datagen = ImageDataGenerator(rescale=1/255.0)
             width, height = 150, 150
             testDatagen = test_datagen.flow_from_dataframe(test, directory = './test', x_col='path', class_mode= None,
                                                        target_size=(width,height), batch_size = 16, shuffle=False)
             
             ```
             
             
             ## 3. Models Training & Prediction
             ### 3.1. Convolutional Network
             
             ```python
             model = models.Sequential()
             model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(150,150,3)))
             model.add(layers.MaxPooling2D((2,2)))
              
             model.add(layers.Conv2D(64,(3,3),activation='relu'))
             model.add(layers.MaxPooling2D((2,2)))
             model.add(layers.BatchNormalization())
             model.add(layers.Dropout(0.3))
                 
             model.add(layers.Conv2D(128,(3,3),activation='relu'))
             model.add(layers.MaxPooling2D((2,2)))
              
             model.add(layers.Conv2D(128,(3,3),activation='relu'))
             model.add(layers.MaxPooling2D((2,2)))
             model.add(layers.BatchNormalization())
             model.add(layers.Dropout(0.3))
             
             model.add(layers.Flatten())
             
             model.add(layers.Dense(256,activation='relu'))
             model.add(layers.Dense(2,activation='softmax'))
             
             model.compile(loss="categorical_crossentropy",optimizer=optimizers.Adam(learning_rate=1e-4),metrics=['acc'])
              
             model.summary()
             
             history = model.fit(trainDatagen, steps_per_epoch = len(trainDatagen), epochs=10, validation_data = valDatagen, validation_steps=len(valDatagen), shuffle=True)
             
             predictions = model.predict(testDatagen, batch_size=32, verbose =1)
             predictions
             
             submission = pd.read_csv('../input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv')
             submission['label'] = predictions[:,0]
             submission.to_csv('submission_cnn_epoch10.csv', index=False)
             submission.head()
             ```
             
             ```python
             history = model.fit(trainDatagen, steps_per_epoch = len(trainDatagen), epochs=15, validation_data = valDatagen, validation_steps=len(valDatagen), shuffle=True)
             
             predictions1 = model.predict(testDatagen, batch_size=32, verbose =1)
             predictions1
             
             submission1 = pd.read_csv('../input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv')
             submission1['label'] = predictions1[:,0]
             submission1.to_csv('submission_ep15_1.csv', index=False)
             submission1.head()
             ```
             
             ## 3.2. Transfer learning Xception model
             
             ```python
             images_size = 150
             batch_size = 32
             
             base_model = Xception(weights='imagenet', include_top=False, input_shape=(images_size, images_size, 3))
             for layer in base_model.layers:
                 layer.trainable = False
             
             model = models.Sequential([
                 base_model,
                 
                 layers.Flatten(),
                 
                 layers.Dense(256,activation='relu'),
                 layers.Dropout(0.5),
                 layers.Dense(2,activation='softmax'),
             ])
             
             model.summary()
             
             learning_rate_schedule = keras.optimizers.schedules.ExponentialDecay(
                 initial_learning_rate=0.01,  # Initial learning rate for training
                 decay_steps=1000,            # Number of steps before decaying the learning rate
                 decay_rate=0.5,              # Rate at which the learning rate decreases
             )
             optimizer = optimizers.Adam(learning_rate=learning_rate_schedule)
             model.compile(optimizer=optimizer,
                          loss="categorical_crossentropy",
                           metrics=['accuracy']
                          )
             from tensorflow.keras.callbacks import LearningRateScheduler
             early_stopping = EarlyStopping(
                 min_delta=0.001, # minimium amount of change to count as an improvement
                 patience=5, # how many epochs to wait before stopping
                 restore_best_weights=True,
             )
             learning_rate_reduce = ReduceLROnPlateau(
                 monitor='val_acc',   # Metric to monitor for changes (usually validation accuracy)
                 patience=5,          # Number of epochs with no improvement after which learning rate will be reduced
                 verbose=1,           # Verbosity mode (0: silent, 1: update messages)
                 factor=0.5,          # Factor by which the learning rate will be reduced (e.g., 0.5 means halving)
                 min_lr=0.00001       # Lower bound for the learning rate (it won't go below this value)
             )
             lr_callback = LearningRateScheduler(learning_rate_schedule)
             callback=[ lr_callback , learning_rate_reduce ,early_stopping ]
             
             
             
             
             # ExponentialDecay for the learning rate
             learning_rate_schedule = keras.optimizers.schedules.ExponentialDecay(
                 initial_learning_rate=0.01,
                 decay_steps=1000,
                 decay_rate=0.5,
             )
             
             # Use this learning rate schedule in the optimizer
             optimizer = optimizers.Adam(learning_rate=learning_rate_schedule)
             
             # Compile the model
             model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=['accuracy'])
             
             # Callbacks (without LearningRateScheduler)
             callbacks = [
                 EarlyStopping(
                     min_delta=0.001,
                     patience=5,
                     restore_best_weights=True,
                 ),
                 # Optionally include ReduceLROnPlateau if you want to use it instead of ExponentialDecay
             ]
             
             # Fit the model
             history = model.fit(
                 trainDatagen,
                 steps_per_epoch=trainDatagen.samples // batch_size,
                 epochs=20,
                 validation_data=valDatagen,
                 validation_steps=valDatagen.samples // batch_size,
                 callbacks=callbacks
             )
             
             
             
             predictions_xcep = model.predict(testDatagen, batch_size=32, verbose =1)
             predictions_xcep
             
             
             submission1 = pd.read_csv('../input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv')
             submission1['label'] = predictions_xcep[:,0]
             submission1.to_csv('submission_xception.csv', index=False)
             submission1.head()
             ```
             
             
             ## 3.3. Stack model with transfer learning and convolutional network
             
             
             ```python
             from tensorflow.keras import layers, models
             from tensorflow.keras.applications import VGG16
             from tensorflow.keras.models import Model
             from tensorflow.keras.layers import concatenate
             
             
             # Load VGG16 model
             vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
             
             # Custom CNN Model
             custom_cnn = models.Sequential([
                 layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),
                 layers.MaxPooling2D((2, 2)),
                 layers.Conv2D(64, (3, 3), activation='relu'),
                 layers.MaxPooling2D((2, 2)),
                 layers.BatchNormalization(),
                 layers.Dropout(0.3),
                 layers.Conv2D(128, (3, 3), activation='relu'),
                 layers.MaxPooling2D((2, 2)),
                 layers.Conv2D(128, (3, 3), activation='relu'),
                 layers.MaxPooling2D((2, 2)),
                 layers.BatchNormalization(),
                 layers.Dropout(0.3),])
             
             # Feature extraction
             # Add GlobalAveragePooling2D to both models
             vgg_output = GlobalAveragePooling2D()(vgg16.output)
             custom_cnn_output = GlobalAveragePooling2D()(custom_cnn.output)
             
             # Concatenate features
             combined = layers.concatenate([vgg_output, custom_cnn_output])
             
             
             # Additional layers
             x = layers.Flatten()(combined)
             x = layers.Dense(1024, activation='relu')(x)
             #-- output = layers.Dense(10, activation='softmax')(x)  # Adjust number of units based on your problem
             output = layers.Dense(2, activation='sigmoid')(x)
             
             
             # Combined model
             model = Model(inputs=[vgg16.input, custom_cnn.input], outputs=output)
             
             # Compile and train
             model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])# Fit the model
             
             
             def generate_dual_input(generator):
                 for (inputs, labels) in generator:
                     yield [inputs, inputs], labels
             
             # Create dual-input generators
             train_dual_gen = generate_dual_input(trainDatagen)
             val_dual_gen = generate_dual_input(valDatagen)
             
             
             history = model.fit(
                 train_dual_gen,
                 steps_per_epoch=len(trainDatagen),
                 validation_data=val_dual_gen,
                 validation_steps=len(valDatagen),
                 epochs=10  # Adjust as needed
             )
             
             def generate_dual_input_test(generator):
                 for inputs in generator:
                     yield [inputs, inputs]
             
             
             test_dual_gen = generate_dual_input_test(testDatagen)
             predictions_stacked = model.predict(test_dual_gen, steps=len(testDatagen), verbose=1)
             
             submission1 = pd.read_csv('../input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv')
             submission1['label'] = predictions_stacked[:,0]
             submission1.to_csv('submission_stacked.csv', index=False)
             submission1.head()
             
             
             ```
             


