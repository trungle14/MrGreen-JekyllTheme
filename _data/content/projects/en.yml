# you can move this content to front matter of [language]/tabs/projects.md
###########################################################
#                Projects Page Data
###########################################################
page_data:
  main:
    header: "Data Science Projects"
    info: "Our job is extract signal from noise"
    text_color: "white"
    # if you don't want to use background image, comment it. back_color will be activated.
    img: ":projects-heading.jpg"
    back_color: "lightblue"

  category:
    - title: "Example"
      type: id_example
      color: "gray"
    - title: "Picture"
      type: id_picture
      color: "#62b462"
    - title: "Quote"
      type: id_quote
      color: "#2FD0ED"

  list:
    # example
    - type: id_example
      project_name: "Inventory Management and Demand Forecasting by Google Cloud Platform"
      project_excerpt: "Role: Technical Lead"
      img: ":project1_thumb.jpg"
      img_title: "img title1"
      date: "2021-03-13"
      post: |

        # Abstract: 
        The project introduces a comprehensive solution for enhancing inventory management in the e-commerce domain. Leveraging the robust capabilities of Google Cloud Platform, our system integrates Big Data and Machine Learning to optimize inventory levels. The core of our analytics pipeline resides in Google BigQuery, where data summarization and analysis take place, providing dynamic insights into inventory availability and accurate demand forecasting. The culmination of our efforts is a user-friendly dashboard developed on Google's Looker Studio. This platform empowers end-users and clients to intuitively visualize product inventory counts and forecasts, facilitating informed decision-making and efficient inventory control.

        # Dataset 

        The [eCommerce dataset](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce?project=galvanic-portal-404814) contains information about customers, products, orders, logistics, web events and digital marketing campaigns. The contents of this dataset are synthetic, and are provided to industry practitioners for the purpose of product discovery, testing, and evaluation.
        # Improve inventory management with sales forecasting deployed by BigQuery ML and AI Platform Training & Prediction

        In the e-commerce sector, effectively managing inventory is crucial: you need to maintain the right balance, ensuring you have enough stock without overstocking. For large e-commerce companies, this involves managing inventory levels for a vast array of products.

        To guide inventory decisions, historical sales data is invaluable. By analyzing past customer purchase patterns, you can predict future buying trends, helping to determine optimal inventory levels. Time series forecasting is an essential tool in this process.

        For data science teams supporting e-commerce inventory management, this entails generating numerous forecasts and managing the infrastructure required for machine learning (ML) model development and deployment. However, by utilizing BigQuery ML in Google Cloud Platform (GCP), you can streamline this process. BigQuery ML allows you to train, evaluate, and deploy ML models directly using SQL statements, bypassing the need for a separate ML infrastructure. This approach can significantly save time and resources in your e-commerce sales forecasting project.



        # Set up time series modeling works in BigQuery ML
        A time series model with BigQuery ML, multiple components are involved, including an Autoregressive integrated moving average (ARIMA) model. For this project, The BigQuery ML model creation pipeline uses the following components:

        Pre-processing: Automatic cleaning adjustments to the input time series, inspect the most appropriate training data which addresses issues like missing values, duplicated timestamps. There are multiple levels we can also consider for time series, easily customized function on demand:
        1. Product category level
        2. Specific products level
        3. Store level

        Holiday effects: Time series modeling in BigQuery ML can also account for holiday effects. By default, holiday effects modeling is disabled. But since this data is from the United States, and the data includes a minimum one year of daily data, you can also specify an optional HOLIDAY_REGION. 
        Seasonal and trend decomposition using the Seasonal and Trend decomposition using Loess (STL) algorithm. Seasonality extrapolation using the double exponential smoothing (ETS) algorithm.
        Trend modeling using the ARIMA model and the auto.ARIMA algorithm for automatic hyper-parameter tuning. In auto.ARIMA, dozens of candidate models are trained and evaluated in parallel. The best model comes with the lowest Akaike information criterion (AIC).
        We can use a single SQL statement to train the model to forecast a single product or to forecast multiple products at the same time. For more information, see The CREATE MODEL statement for time series models.



        # Deployment quick guide:
        1. Create a new project on GCP platform
        2. Enable BigQuery, Vertex AI, AI platform, LookerStudio services and create new instance and new notebook with python kernel -- You are ready to run sales forecast
        3. Prepare training dataset
        3.1. Upload/Import dataset in Cloud storage and BigQuery, create training dataset under your project or
        3.2. Enable Streamming by Dataflow for data training preparation
        4. Develop and train model followed by making prediction
        5. Export output as predicted sales for further usage as monitoring dashboard

        Here is the example how we can integrate forecasted sales into current dashboard:

        <img src="[https://i.ibb.co/RhC6V4R/googletrends-81-1-1.png](https://ibb.co/51NCZF8"><img src="https://i.ibb.co/D1xmdpC/Picture1.png)" alt=""></a>
          </td>



        ## Suggestion for further deployment
        1. Adding lagging and rolling features to the model for each training window
        2. Considering using advanced models like XGBoost or LightGBM 
        3. Adding insightful features like Top 25 Rising queries from **Google Trends** <td>
        <a href="https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/google-search-trends?_ga=2.261190030.2019434361.1656948847-1975246695.1656948843&project=galvanic-portal-404814">
          <img src="https://i.ibb.co/RhC6V4R/googletrends-81-1-1.png" alt=""></a>[dataset Google trends on Google BigQuery](https://console.cloud.google.com/marketplace/product/bigquery-public-datasets/google-search-trends?_ga=2.261190030.2019434361.1656948847-1975246695.1656948843&project=galvanic-portal-404814)
            

        ## Docoments
        The [Presentation Recording:](https://drive.google.com/file/d/1Roq84Chgowoh7PfVIuHarL1su3gnZMOA/view?usp=drive_link) and the open-source [Interactive dashboard:](https://lookerstudio.google.com/reporting/ebea5752-963e-4a55-aa45-ebb846519557/page/p_057twy79bd?s=nIzjhrMlO8g)
        the flyers and technical document of the project can be found [here] (https://github.com/trungle14/GoogleCloud_InventoryManagement/tree/main) 

    # example
    - type: id_quote
      project_name: "Walmart Retail Sales Forecasting - M5 Accuracy Competition"
      project_excerpt: "Model used: LightGBM, XGBoost, Deep Neural Network"
      img: ":project1_thumb.jpg"
      img_title: "img title5"
      date: "2023-12-05"
      post: |
         ## 1	Overview
         ### 1.1	Description of Project

          This is a prediction problem  project for Walmart (A Top Retail Group) Sales dataset from Kaggle for the unit sales forecasting. Advanced and comprehensive analytics skills, including Exploratory Data Analysis and Machine Learning Data Prediction Analysis techniques will be used in this case for generating data-driven business insights.

          ### 1.2	Business Context


          In the dynamic landscape of the retail industry, the ability to predict sales accurately is paramount for sustaining and enhancing business operations. For a retail giant like Walmart, whose vast operations span a multitude of products, locations, and customer segments, the challenge of forecasting sales becomes even more intricate.
          Challenges and Risks:
          Walmart confronts the formidable task of maximizing decision-making efficiency amid a sea of data. The stakes are high, as inaccurate predictions can lead to substantial losses. Traditional prediction methods, once reliable, now struggle to cope with the complexities of modern retail dynamics. To avoid costly mistakes and enhance forecasting accuracy, there is a pressing need for the integration of cutting-edge data science techniques.
          Business Imperatives:
          Precise sales predictions stand as the linchpin in Walmart's strategy to navigate both realized and potential revenue opportunities. Efficient inventory management, customer satisfaction, strategic promotions, and a competitive edge hinge on the ability to foresee market trends accurately.
          Benefits of Sales Prediction:\
          (1)	Efficient Inventory Management: Anticipate demand trends, reducing stockouts and overstocks.\
          (2)	Customer Satisfaction: Ensure product availability, meeting customer expectations.\
          (3)	Smart Promotions: Strategically plan promotions based on predictive insights.\
          (4)	Competitive Edge: Stay ahead by responding swiftly to market shifts.\
          (5)	Optimized Supply Chain: Streamline operations for cost-effective supply chain management.\
          (6)	Support for Strategic Decisions: Informed decision-making for sustained growth.\
          (7)	Reduce Financial Risks: Improve budget management efficiency through accurate sales forecasts.\
          (8)	Raise Shareholder Confidence: Provide stakeholders with reliable projections, enhancing trust.

          **Situation:**
          Walmart is at the nexus of leveraging its rich dataset to drive decision-making efficiency. The precision of sales predictions becomes pivotal, steering the company away from both tangible and missed revenue opportunities.
          Key Question:
          How can Walmart forecast daily sales for the next 28 days, leveraging hierarchical sales data effectively?
          Proposed Solution:
          The proposed solution involves harnessing the power of machine learning to predict future sales. By embracing advanced analytics, we  aim to enhance the  forecast accuracy for Walmart, ensuring a proactive and data-driven approach to sales management.

          This strategic integration of data science not only addresses current challenges but positions Walmart at the forefront of innovative and efficient retail practices, fostering sustained growth and market leadership.

          ## 2. Data Description & Exploratory Data Analysis

          <img width="670" alt="Screenshot 2024-01-04 at 19 59 54" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/ca4cde95-c4f4-4399-8041-071ab7ac8683">

          This table shows the overview of the Input Data:
          Raw Data	Description	# Feature	# Record	Data Size
          calendar.csv	Workday & Special event day (e.g. SuperBowl)	14	1.9 K	103 kB 
          sell_prices.csv	Price of the products sold per store and date	4	6.84 M	203.4 MB
          sales_train_validation.csv	historical daily unit sales data per product and store [d1 - d1913]	1019	30.5 K	120 MB
          sales_train_evaluation.csv	sales [d1 - d_1941]	1047	30.5 K	121.7 MB
          Based on the structure of data we see the data would be of below format:

          <img width="685" alt="Screenshot 2024-01-04 at 20 00 36" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/d5acb20b-a021-4245-8f3d-27e581c4b1a9">


          ## 3. Methodology 


          <img width="660" alt="Screenshot 2024-01-04 at 22 49 07" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/56702eff-a1f8-4fbe-ad85-ceb02ba3dde6">





          **3.3.1. Price feature**\
          We are doing feature engineering here to get price related data, we have week wise data of price (we have price features for test weeks as well).
          We are using expanding max price , minimum price , standard deviation , mean, so that there is no data   leakage from future to past, and ,model can solely use the past data using expanding method (since the data is already sorted time wise we are not sorting again , saves in computation time).


          **3.3.2. Calendar features**\
          we see prices of some items starting for a particular week, which might indicate that would be release week for the product so we can use data in base data frame after that point (as since earlier data was in long format it would have data for all items through all days)
          This reduces the size of the data and will have features of when the product was released (capturing any trends if items get sold when we are predicting for volumes closer to release dates). Then we do label encoding of the categorical features so that they can be used for regression algorithms


          **3.3.3. Lag and rolling lags features**\
          Another important feature we observed in winning solutions is they used lags data and roll data in feature engineering. This gives how trends data could be captured using a regression algorithm , though we are not specifically using time series data.
          For this we have considered rolling sum of the number of times, 0 units of product were sold, 7, 14, 30, 60, 180 days of roll (week, 2 weeks, approx month, 2 months approx, approx half year), with this we will be able to capture trend details.
          As next important features we have chosen lag features (these will capture sales with a lag of that many days we have in feature.


          **3.3.4. Categories - Item, Store, Department, State Level Features**\
          We then use category wise sales data, item wise sales data, department wise sales data (across all stores), then also use store and category wise sales data, store and item wise sales data, store and department wise sales data. This gives cross sectional features that our model could pick if there is any trend.



          **4. Model Training and Prediction**\

          **Train and Predict**\
          First we need to model comparison to see which model produces a better kaggle score and use that model , then optimize the step size so as to improve the score further.
          through this process we are basically using the sales data that we have on t- step (for example during model selection here for 1- 14 days prediction step will be 14, and from 14 till 28th day step will be 28 days)
          Then we run a prediction model where we first loop over store and department to train the model (slow) , next over store and category (will be quicker) and take average of both the methods to arrive at final submission.



          **LightGBM**\
          <img width="1080" alt="Screenshot 2024-01-06 at 22 29 14" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/6f93c326-e9eb-41cf-b1db-3ce0eec7ffaf">

          **Extreme Gradient Boosting - XGBOOST**\
          <img width="1084" alt="Screenshot 2024-01-06 at 22 30 52" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/19b07cc1-9c98-4cd2-9cd8-46d53e6599f6">


          **Neural network**\
          <img width="784" alt="Screenshot 2024-01-06 at 22 37 13" src="https://github.com/trungle14/WalmartSalesForecasting/assets/143222481/86d7c335-5570-47e0-98b1-4ec673617643">






          | Models    | Hyperparameters                           | Kaggle Score |
          |-----------|-------------------------------------------|--------------|
          | LightGBM  | [See Hyperparameters](#lightgbm-parameters)| 0.5302        |
          | XGBoost   | [See Hyperparameters](#xgboost-parameters)| 0.5599      |
          | Neural Netwwork| [See Hyperparameters](#lightgbm-parameters)| 0.728 |

          ## LightGBM Parameters

          ```python
          lgb_params = {
              'boosting_type': 'gbdt',
              'objective': 'tweedie',
              'tweedie_variance_power': 1.1,
              'metric': 'rmse',
              'subsample': 0.5,
              'subsample_freq': 1,
              'min_child_weight': 1,
              'learning_rate': 0.03,
              'num_leaves': 2 ** 11 - 1,
              'min_data_in_leaf': 2 ** 12 - 1,
              'feature_fraction': 0.5,
              'max_bin': 100,
              'n_estimators': 1400,
              'boost_from_average': False,
              'verbosity': -1
                              }
          Lgbm = LGBMRegressor(**lgb_params)
          callbacks = [early_stopping(stopping_rounds=50, first_metric_only=False)]
          ```



          ## XGBoost Parameters

          ```python
           # Train

          model = tf.keras.models.Sequential([
          tf.keras.layers.Dense(64, activation='relu', input_shape=(trainX.shape[1],)),
          tf.keras.layers.Dense(1, activation='linear') ]) # Linear activation for regression
                                          

          # Compile the model
          model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])

          # Display the model summary
          #model.summary()
          # Train the model
          history = model.fit(trainX, trainY, epochs=5, batch_size=32, validation_data=(valX, valY))

          # Make predictions on the test set
          yhat = model.predict(testX).flatten()

          preds = grid[(grid['d'] >= pred_start) & (grid['d'] <= pred_end)][['id', 'd']]
          preds['sales'] = yhat
          predictions = pd.concat([predictions, preds], axis=0)
          ```



          xgb_params = {
              'objective': 'reg:tweedie',  
              'eval_metric': 'rmse', 
              'subsample': 0.5,
              'colsample_bytree': 0.5,
              'learning_rate': 0.03,
              'max_depth': 11,  
              'min_child_weight': 4096,  
              'n_estimators': 1400,
              'max_bin': 100,
              'seed': 42
                    }





